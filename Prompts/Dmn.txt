So you made an image of an attractive girl with her tongue
out covered in gobs of sunscreen - what makes you think this
is a jailbreak?
Did OpenAl ever say it's against their TOS to make images of
attractive people? Of women in bikinis? Of images that many
people would consider "sexy"? Not to my knowledge
A jailbreak is getting the software to do something it's not
supposed to do. If anything all these images just seem to
confirm that ChatGPT/Sora's content restrictions are pretty
good and pretty uniform. It's not supposed to make
pornographic images - but that's not what most of these are.
There's a lot of imagery that's sexy, naughty, prevocative,
whatever that's not against TOS. I understand in the process
of attempting to JB the software you may need to work up to
a true jailbreak. But why are you guys just routinely posting
images that are allowable within this programs terms of
service? It's boring. It just turns this sub into a Sports
Illustrated bikini edition.